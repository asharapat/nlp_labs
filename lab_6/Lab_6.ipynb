{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nurbekzhussip/nlp_labs/blob/master/lab_6/Lab_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bSyLRsM1fCX",
        "colab_type": "code",
        "outputId": "86a1b668-fe09-432e-d618-f33b54a5562d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "import json\n",
        "\n",
        "with open('dataset_43428_1.txt', encoding=\"utf-8\") as json_file:\n",
        "    dataset = json.load(json_file)\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from itertools import combinations\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, RegexpTokenizer,word_tokenize\n",
        "from nltk.stem.snowball import RussianStemmer\n",
        "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
        "from pymystem3 import Mystem\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import re, string, random\n",
        "\n",
        "\n",
        "\n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue \n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        " \n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "\n",
        "stemmer = RussianStemmer()\n",
        "mystem = Mystem()\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words_ru = stopwords.words('russian')\n",
        "stop_words_ru.extend(['что', 'но','это','однако', 'так', 'вот', 'быть', 'как', 'в', '—', '–', 'к', 'на', '...'])\n",
        "\n",
        "cleared_text=[]\n",
        "cleared_sentences = []\n",
        "cleared_words = []\n",
        "\n",
        "for text in dataset:\n",
        "  sentences_list = sent_tokenize(text)\n",
        "\n",
        "  for sentences in sentences_list:  \n",
        "    words = word_tokenize(sentences)\n",
        "    for word in words:\n",
        "      if (word not in stop_words_ru):\n",
        "        cleared_words.append(mystem.lemmatize(word))\n",
        "        cleared_words.append(word)\n",
        "    cleared_sentences.append(' '.join(cleared_words))\n",
        "    cleared_words = []\n",
        "      \n",
        "  cleared_text.append(' '.join(cleared_sentences))\n",
        "  cleared_sentences=[]\n",
        "\n",
        "summarize_all_text =[]\n",
        "summarize_text =[]\n",
        "\n",
        "for text in dataset:\n",
        "  sentences_list = sent_tokenize(text)\n",
        "  sentence_similarity_martix = build_similarity_matrix(sentences_list, stop_words_ru)\n",
        "\n",
        "  # Step 3 - Rank sentences in similarity martix\n",
        "  sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "  scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "\n",
        "  # Step 4 - Sort the rank and pick top sentences\n",
        "  ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences_list)), reverse=True)    \n",
        "  #print(\"Indexes of top ranked_sentence order are \", ranked_sentence)  \n",
        "\n",
        "  top_n = round(len(sentences_list)/2)\n",
        "\n",
        "  for i in range(top_n):\n",
        "    summarize_text.append(ranked_sentence[i][1])\n",
        "  \n",
        "  summarize_all_text.append(summarize_text)\n",
        "  summarize_text = []\n",
        "\n",
        "with open('results.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(summarize_all_text, f, ensure_ascii=False, indent=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/nlpub/pymystem3\n",
            "  Cloning https://github.com/nlpub/pymystem3 to /tmp/pip-req-build-1a1i9_9l\n",
            "  Running command git clone -q https://github.com/nlpub/pymystem3 /tmp/pip-req-build-1a1i9_9l\n",
            "Requirement already satisfied (use --upgrade to upgrade): pymystem3==0.2.0 from git+https://github.com/nlpub/pymystem3 in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pymystem3==0.2.0) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.2.0) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.2.0) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.2.0) (1.24.3)\n",
            "Building wheels for collected packages: pymystem3\n",
            "  Building wheel for pymystem3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymystem3: filename=pymystem3-0.2.0-cp36-none-any.whl size=9921 sha256=fb41821098f63de29401f75cf99c408e26ba1b9c03051d2c6bd6f79049432ee8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s0zi6fff/wheels/7d/75/c2/216a594291dee680749ce12c60d16125cfe1f363059e7163dc\n",
            "Successfully built pymystem3\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}